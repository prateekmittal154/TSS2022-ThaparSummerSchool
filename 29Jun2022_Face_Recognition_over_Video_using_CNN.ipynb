{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "29Jun2022_Face_Recognition_over_Video_using_CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP9QGQRlcrV/6SZgwxE7oE6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahil301290/TSS2022-ThaparSummerSchool/blob/main/29Jun2022_Face_Recognition_over_Video_using_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Following three tasks are to be done:**\n",
        "\n",
        "## **1. Facial Similarity using Cosine Similarity**\n",
        "\n",
        "## **2. Facial Recognition using One Shot Learning**\n",
        "\n",
        "## **3. Testing on Video of Friends TV Series**"
      ],
      "metadata": {
        "id": "wu7b79GvxEKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Getting System Information"
      ],
      "metadata": {
        "id": "wt0eaiHUGnzg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3AA72-DGb3T"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import all required libraries"
      ],
      "metadata": {
        "id": "zP2aUCSKG7qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, save_img, img_to_array\n",
        "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "JD2YtwJwGsdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Drive"
      ],
      "metadata": {
        "id": "2bs5DK0Y7F4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "RcSmEary7JIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Unzipping the dataset"
      ],
      "metadata": {
        "id": "4eX_jD02HXEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/TSS2022_Datasets/face_recognition.zip'"
      ],
      "metadata": {
        "id": "3wHO-9-UG_mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define the CNN Model"
      ],
      "metadata": {
        "id": "EraoXog4ID-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
        "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Convolution2D(2622, (1, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "CvcpX-5qHZPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load our VGG Face Weights**\n",
        "\n",
        "We don't need to train our model if we can get the already trained 'weights'."
      ],
      "metadata": {
        "id": "m9u0d2BIL_JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#you can download the pretrained weights from the following link \n",
        "#https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing\n",
        "#or you can find the detailed documentation https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/\n",
        "\n",
        "!gdown --id 14eHppxprE1sCWmnjQ7LuijhAZQlb_Quz"
      ],
      "metadata": {
        "id": "3mG6Amj5L8AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('vgg_face_weights.h5')"
      ],
      "metadata": {
        "id": "ve0zEhajMDcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Facial Similarity using Cosine Similarity**\n",
        "\n",
        "![Image of Cosine Similarity](https://raw.githubusercontent.com/rajeevratan84/DeepLearningCV/master/cosine.JPG)\n",
        "![Image of Cosine Similarity Formula](https://raw.githubusercontent.com/rajeevratan84/DeepLearningCV/master/cosinesim.JPG)"
      ],
      "metadata": {
        "id": "ABH04_YwMUm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(224, 224))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def findCosineSimilarity(source_representation, test_representation):\n",
        "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
        "    b = np.sum(np.multiply(source_representation, source_representation))\n",
        "    c = np.sum(np.multiply(test_representation, test_representation))\n",
        "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
        "\n",
        "# Our model that outputs the 2,622 embedding vector\n",
        "vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)"
      ],
      "metadata": {
        "id": "dgyvkD4YMIt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Verify Facial Similarity**\n",
        "\n",
        "#### **Define our verifyFace function where we load to images of faces and compare them.**\n",
        "\n",
        "We set **epsilon** to be the threshold of whether our two faces are the same person. Setting a lower value makes it more strict with our face matching."
      ],
      "metadata": {
        "id": "iiMsVEUNMaZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.40\n",
        "\n",
        "def verifyFace(img1, img2):\n",
        "    # Get embedding/encoding for face1 and face2\n",
        "    img1_representation = vgg_face_descriptor.predict(preprocess_image('./face_recognition/training_faces/%s' % (img1)))[0,:]\n",
        "    img2_representation = vgg_face_descriptor.predict(preprocess_image('./face_recognition/training_faces/%s' % (img2)))[0,:]\n",
        "    \n",
        "    # Calculate the cosine similarity between the two embeddings\n",
        "    cosine_similarity = findCosineSimilarity(img1_representation, img2_representation)\n",
        "    \n",
        "    f = plt.figure()\n",
        "    f.add_subplot(1,2, 1)\n",
        "    plt.imshow(image.load_img('./face_recognition/training_faces/%s' % (img1)))\n",
        "    plt.xticks([]); plt.yticks([])\n",
        "    f.add_subplot(1,2, 2)\n",
        "    plt.imshow(image.load_img('./face_recognition/training_faces/%s' % (img2)))\n",
        "    plt.xticks([]); plt.yticks([])\n",
        "    plt.show(block=True)\n",
        "    \n",
        "    print(\"Cosine similarity: \",cosine_similarity)\n",
        "    \n",
        "    # If similarity score is less than the epsilon threshold\n",
        "    if(cosine_similarity < epsilon):\n",
        "        print(\"They are same person\")\n",
        "    else:\n",
        "        print(\"They are not same person!\")"
      ],
      "metadata": {
        "id": "6bHuDqtFMXeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Let's a run a few tests**"
      ],
      "metadata": {
        "id": "orIQWiEhMgUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's compare two faces of the same person\n",
        "verifyFace(\"Nidia_1.jpg\", \"Nidia_2.jpg\")"
      ],
      "metadata": {
        "id": "m5qBVpjsMcm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now try it on 2nd picture of the same person\n",
        "verifyFace(\"Nidia_4.jpg\", \"Nidia_6.jpg\")"
      ],
      "metadata": {
        "id": "cmChAp6bMjBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's compare her face to Jennifer Lopez\n",
        "verifyFace(\"Nidia_5.jpg\", \"jlo.jpg\")"
      ],
      "metadata": {
        "id": "qRcch6ayOnAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And now Jennifer Lopez with Lady Gaga\n",
        "verifyFace(\"jlo.jpg\", \"ladygaga.jpg\")"
      ],
      "metadata": {
        "id": "dS7UgN59Op0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Facial Recognition with One Shot Learning**\n",
        "### **Extract faces from pictures of people**\n",
        "\n",
        "#### **Instrutions:**\n",
        "1. Place photos of people (one face visible) in the folder called \"./people\"\n",
        "2. Replace my photo titled \"sahil.jpg\" with a piture of your face for testing on a webcam\n",
        "3. Faces are extracted using the haarcascade_frontalface_default detector model\n",
        "4. Extracted faces are placed in the folder called \"./group_of_faces\"\n",
        "5. We are extracting the faces needed for our one-shot learning model, it will load 5 extracted faces"
      ],
      "metadata": {
        "id": "IkkGWvTiOzXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1_X-V1Lp6qMAl_-9opsseieprD3Lhdq8U\n",
        "!unzip -qq haarcascades.zip\n",
        "!rm -rf people/.DS_Store"
      ],
      "metadata": {
        "id": "Ky5VntILOsYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our Setup, Import Libaries, Create our Imshow Function and Download our Images\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Define our imshow function \n",
        "def imshow(title = \"Image\", image = None, size = 8):\n",
        "      w, h = image.shape[0], image.shape[1]\n",
        "      aspect_ratio = w/h\n",
        "      plt.figure(figsize=(size * aspect_ratio,size))\n",
        "      plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "      plt.title(title)\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "Qj8hGehuO-fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below extracts faces from images and places them in the folder\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "# Create a function to setup the directories we'll be storing our images\n",
        "def makedir(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "        return None\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "# Loading out HAARCascade Face Detector \n",
        "face_detector = cv2.CascadeClassifier('Haarcascades/haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Directory of image of persons we'll be extracting faces frommy\n",
        "mypath = \"./face_recognition/people/\"\n",
        "image_file_names = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "print(image_file_names)\n",
        "print(\"Collected \" + str(len(image_file_names)) + \" images\")\n",
        "makedir(\"./face_recognition/group_of_faces/\")\n",
        "\n",
        "for image_name in image_file_names:\n",
        "    person_image = cv2.imread(mypath+image_name)\n",
        "    face_info = face_detector.detectMultiScale(person_image, 1.3, 5)\n",
        "    for (x,y,w,h) in face_info:\n",
        "        face = person_image[y:y+h, x:x+w]\n",
        "        roi = cv2.resize(face, (128, 128), interpolation = cv2.INTER_CUBIC)\n",
        "    \n",
        "    path = \"./face_recognition/group_of_faces/\" + \"face_\" + image_name\n",
        "    cv2.imwrite(path, roi)\n",
        "    imshow(\"face\", roi)"
      ],
      "metadata": {
        "id": "cJrbMTTIPGiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load our VGGFaceModel**\n",
        "- This block of code defines the VGGFace model (which we use later) and loads the model"
      ],
      "metadata": {
        "id": "4DSRvvsZsQmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#you can find the documentation of this code from the following link: https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import load_img, save_img, img_to_array\n",
        "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"Loads image from path and resizes it\"\"\"\n",
        "    img = load_img(image_path, target_size=(224, 224))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "model = Sequential()\n",
        "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
        "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Convolution2D(2622, (1, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "#you can download pretrained weights from https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing\n",
        "from tensorflow.keras.models import model_from_json\n",
        "model.load_weights('vgg_face_weights.h5')\n",
        "\n",
        "vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)\n",
        "\n",
        "model = vgg_face_descriptor\n",
        "\n",
        "print(\"Model Loaded\")"
      ],
      "metadata": {
        "id": "J0SLOvw1sRGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test model using your Webcam**\n",
        "This code looks up the faces you extracted in the \"group_of_faces\" folder and uses the similarity (Cosine Similarity) to detect which faces is most similar to the one being extracted with your webcam."
      ],
      "metadata": {
        "id": "N-cBr405sgAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "metadata": {
        "id": "zcIayZmhschw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "metadata": {
        "id": "IP6CzatyskD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#points to your extracted faces\n",
        "people_pictures = \"./face_recognition/group_of_faces/\"\n",
        "\n",
        "all_people_faces = dict()\n",
        "\n",
        "for file in listdir(people_pictures):\n",
        "    person_face, extension = file.split(\".\")\n",
        "    try:\n",
        "      all_people_faces[person_face] = model.predict(preprocess_image('./group_of_faces/%s.jpg' % (person_face)))[0,:]\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "print(\"Face representations retrieved successfully\")\n",
        "\n",
        "def findCosineSimilarity(source_representation, test_representation):\n",
        "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
        "    b = np.sum(np.multiply(source_representation, source_representation))\n",
        "    c = np.sum(np.multiply(test_representation, test_representation))\n",
        "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
        "\n",
        "img = cv2.imread('photo.jpg')\n",
        "faces = face_detector.detectMultiScale(img, 1.3, 5)\n",
        "\n",
        "for (x,y,w,h) in faces:\n",
        "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) #draw rectangle to main image\n",
        "    detected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
        "    detected_face = cv2.resize(detected_face, (224, 224)) #resize to 224x224\n",
        "\n",
        "    img_pixels = image.img_to_array(detected_face)\n",
        "    img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
        "    img_pixels /= 255\n",
        "\n",
        "    captured_representation = model.predict(img_pixels)[0,:]\n",
        "\n",
        "    found = 0\n",
        "    for i in all_people_faces:\n",
        "        person_name = i\n",
        "        representation = all_people_faces[i]\n",
        "\n",
        "        similarity = findCosineSimilarity(representation, captured_representation)\n",
        "        if(similarity < 0.35):\n",
        "            cv2.putText(img, person_name[5:], (int(x+w+15), int(y-12)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "            found = 1\n",
        "            break\n",
        "\n",
        "    #connect face and text\n",
        "    cv2.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),(255, 0, 0),1)\n",
        "    cv2.line(img,(x+w,y-20),(x+w+10,y-20),(255, 0, 0),1)\n",
        "\n",
        "    if(found == 0): #if found image is not in our people database\n",
        "        cv2.putText(img, 'unknown', (int(x+w+15), int(y-12)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
        "\n",
        "imshow('img',img)"
      ],
      "metadata": {
        "id": "F-LJDxLwslwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Test on video from the Friends TV show**\n",
        "\n",
        "Since we're using the Friends TV Series characters, let's extract the faces from the images I placed in the \"./friends\" folder"
      ],
      "metadata": {
        "id": "BBOgL18StFX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!find . -name '.DS_Store' -type f -delete"
      ],
      "metadata": {
        "id": "KpRihBfhs0JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "\n",
        "# Loading out HAARCascade Face Detector \n",
        "face_detector = cv2.CascadeClassifier('Haarcascades/haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Directory of image of persons we'll be extracting faces frommy\n",
        "mypath = \"./face_recognition/friends/\"\n",
        "image_file_names = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "print(\"Collected image names\")\n",
        "makedir(\"friends_faces/\")\n",
        "\n",
        "for image_name in image_file_names:\n",
        "    person_image = cv2.imread(mypath+image_name)\n",
        "    face_info = face_detector.detectMultiScale(person_image, 1.3, 5)\n",
        "    for (x,y,w,h) in face_info:\n",
        "        face = person_image[y:y+h, x:x+w]\n",
        "        roi = cv2.resize(face, (128, 128), interpolation = cv2.INTER_CUBIC)\n",
        "    path = \"friends_faces/\" + \"face_\" + image_name \n",
        "    cv2.imwrite(path, roi)\n",
        "    imshow(\"face\", roi)"
      ],
      "metadata": {
        "id": "CIy-W9PhtH92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we load our faces from the \"friends_faces\" directory and we run our face classifier model our test video"
      ],
      "metadata": {
        "id": "hi4VjgFGtRMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#points to your extracted faces\n",
        "people_pictures = \"./friends_faces/\"\n",
        "\n",
        "all_people_faces = dict()\n",
        "\n",
        "for file in listdir(people_pictures):\n",
        "    person_face, extension = file.split(\".\")\n",
        "    try:\n",
        "      all_people_faces[person_face] = model.predict(preprocess_image('./face_recognition/friends_faces/%s.jpg' % (person_face)))[0,:]\n",
        "    except:\n",
        "      pass\n",
        "      \n",
        "print(\"Face representations retrieved successfully\")\n",
        "\n",
        "def findCosineSimilarity(source_representation, test_representation):\n",
        "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
        "    b = np.sum(np.multiply(source_representation, source_representation))\n",
        "    c = np.sum(np.multiply(test_representation, test_representation))\n",
        "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
        "\n",
        "cap = cv2.VideoCapture('./face_recognition/Friends.mp4')\n",
        "frame_count = 0\n",
        "\n",
        "# Get the height and width of the frame (required to be an integer)\n",
        "w = int(cap.get(3)) + 200\n",
        "h = int(cap.get(4)) + 200\n",
        "\n",
        "# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\n",
        "out = cv2.VideoWriter('friends_face_recognition.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 30, (w, h))\n",
        "\n",
        "while(True):\n",
        "  ret, img = cap.read()\n",
        "  if ret:\n",
        "    #img = cv2.resize(img, (320, 180)) # Re-size video to as smaller size to improve face detection speed\n",
        "    img = cv2.copyMakeBorder(img, 100, 100, 100, 100, cv2.BORDER_CONSTANT, value=[0,0,0])\n",
        "    faces = face_detector.detectMultiScale(img, 1.3, 5)\n",
        "    frame_count+=1\n",
        "    for (x,y,w,h) in faces:\n",
        "      if w > 13: \n",
        "          cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) #draw rectangle to main image\n",
        "\n",
        "          detected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
        "          detected_face = cv2.resize(detected_face, (224, 224)) #resize to 224x224\n",
        "\n",
        "          img_pixels = image.img_to_array(detected_face)\n",
        "          ls = image.img_to_array(detected_face)\n",
        "          img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
        "          img_pixels /= 255\n",
        "\n",
        "          captured_representation = model.predict(img_pixels)[0,:]\n",
        "\n",
        "          found = 0\n",
        "          for i in all_people_faces:\n",
        "            person_name = i\n",
        "            representation = all_people_faces[i]\n",
        "\n",
        "            similarity = findCosineSimilarity(representation, captured_representation)\n",
        "            if(similarity < 0.30):\n",
        "                cv2.putText(img, person_name[5:], (int(x+w+15), int(y-12)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "                found = 1\n",
        "                break\n",
        "\n",
        "            #connect face and text\n",
        "            cv2.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),(255, 0, 0),1)\n",
        "            cv2.line(img,(x+w,y-20),(x+w+10,y-20),(255, 0, 0),1)\n",
        "\n",
        "    imshow('img',cv2.resize(img, (640, 360)))\n",
        "    # Write the frame into the file 'output.avi'\n",
        "    out.write(img)\n",
        "  else:\n",
        "    break\n",
        "\n",
        "cap.release()\n",
        "out.release()"
      ],
      "metadata": {
        "id": "yLB_1C0NtKfc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}