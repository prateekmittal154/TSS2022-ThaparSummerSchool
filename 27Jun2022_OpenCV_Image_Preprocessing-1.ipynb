{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "27Jun2022_OpenCV_Image_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTHUAT20f7WdKxl5Y8JqSQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahil301290/TSS2022-ThaparSummerSchool/blob/main/27Jun2022_OpenCV_Image_Preprocessing-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTL1e0Q-nVyA"
      },
      "outputs": [],
      "source": [
        "# Our Setup, Import Libaries, Create our Imshow Function and Download our Images\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Define our imshow function \n",
        "def imshow(title = \"Image\", image = None, size = 10):\n",
        "    w, h = image.shape[0], image.shape[1]\n",
        "    aspect_ratio = w/h\n",
        "    plt.figure(figsize=(size * aspect_ratio,size))\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Download and unzip our images\n",
        "!unzip 'sahil.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Image Transformations**\n",
        "\n",
        "1. Perform Image Translations\n",
        "2. Rotations with getRotationMatrix2D\n",
        "3. Rotations with Transpose\n",
        "4. Flipping Images"
      ],
      "metadata": {
        "id": "KPIf7-E5tya2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Translations**\n",
        "\n",
        "This an affine transform that simply shifts the position of an image. (left or right).\n",
        "\n",
        "We use cv2.warpAffine to implement these transformations.\n",
        "\n",
        "```cv2.warpAffine(image, T, (width, height))```\n",
        "\n",
        "![](https://raw.githubusercontent.com/rajeevratan84/ModernComputerVision/main/warp.png)"
      ],
      "metadata": {
        "id": "5FEwLGyhrsxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our image\n",
        "image = cv2.imread('./sahil/sahil_4.jpg')\n",
        "imshow(\"Original\", image)\n",
        "\n",
        "# Store height and width of the image\n",
        "height, width = image.shape[:2]\n",
        "\n",
        "# We shift it by quarter of the height and width\n",
        "quarter_height, quarter_width = height/4, width/4\n",
        "\n",
        "# Our Translation\n",
        "#       | 1 0 Tx |\n",
        "#  T  = | 0 1 Ty |\n",
        "\n",
        "# T is our translation matrix\n",
        "T = np.float32([[1, 0, quarter_width], [0, 1,quarter_height]])\n",
        "\n",
        "# We use warpAffine to transform the image using the matrix, T\n",
        "img_translation = cv2.warpAffine(image, T, (width, height))\n",
        "imshow(\"Translated\", img_translation)"
      ],
      "metadata": {
        "id": "YHGyTC2frThn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does T look like\n",
        "print(T)\n",
        "\n",
        "print(height, width )"
      ],
      "metadata": {
        "id": "_843ZbsZsAqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Rotations**\n",
        "\n",
        "```cv2.getRotationMatrix2D(rotation_center_x, rotation_center_y, angle of rotation, scale)```\n",
        "\n",
        "![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/rotation.png)\n"
      ],
      "metadata": {
        "id": "R279TdqTsVfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our image\n",
        "image = cv2.imread('./sahil/sahil_4.jpg')\n",
        "height, width = image.shape[:2]\n",
        "\n",
        "# Divide by two to rototate the image around its centre\n",
        "rotation_matrix = cv2.getRotationMatrix2D((width/2, height/2), 90, 1)\n",
        "\n",
        "# Input our image, the rotation matrix and our desired final width and height\n",
        "rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))\n",
        "imshow(\"Rotated 90 degrees with scale = 1\", rotated_image)"
      ],
      "metadata": {
        "id": "-U72vY87sPYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide by two to rototate the image around its centre\n",
        "rotation_matrix = cv2.getRotationMatrix2D((width/2, height/2), 90, 0.5)\n",
        "print(rotation_matrix)\n",
        "# Input our image, the rotation matrix and our desired final width and height\n",
        "rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))\n",
        "imshow(\"Rotated 90 degrees with scale = 0.5\", rotated_image)"
      ],
      "metadata": {
        "id": "GI3elemdsecb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice all the black space surrounding the image.\n",
        "\n",
        "We could now crop the image as we can calculate it's new size (we haven't learned cropping yet!).\n",
        "\n",
        "### **Rotations with cv2.transpose** (less flexible)"
      ],
      "metadata": {
        "id": "5oeziHd1svrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rotated_image = cv2.transpose(image)\n",
        "imshow(\"Original\", image)\n",
        "imshow(\"Rotated using Transpose\", rotated_image)"
      ],
      "metadata": {
        "id": "9Qpy_PlTspLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotated_image = cv2.transpose(image)\n",
        "rotated_image = cv2.transpose(rotated_image)\n",
        "\n",
        "imshow(\"Rotated using Transpose\", rotated_image)"
      ],
      "metadata": {
        "id": "fQQ2LfUtszee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now to a horizontal flip.\n",
        "flipped = cv2.flip(image, 1)\n",
        "imshow(\"Horizontal Flip\", flipped)"
      ],
      "metadata": {
        "id": "33JgrZRzs9xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scaling, Re-sizing, Interpolations and Cropping**\n",
        "\n",
        "1. How to re-size and scale images\n",
        "2. Image Pyramids\n",
        "3. Cropping"
      ],
      "metadata": {
        "id": "ICL9eoAct3Mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Re-sizing**\n",
        "\n",
        "![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/Resizing.png)\n",
        "\n",
        "Re-sizing is a simple function that we execute using the cv2.resize function, it's arguments are:\n",
        "\n",
        "```cv2.resize(image, dsize(output image size), x scale, y scale, interpolation)```\n",
        "- if dsize is None the output image is calculated as a function of scaling using x & y scale \n",
        "\n",
        "#### **List of Interpolation Methods:**\n",
        "- cv2.INTER_AREA - Good for shrinking or down sampling\n",
        "- cv2.INTER_NEAREST - Fastest\n",
        "- cv2.INTER_LINEAR - Good for zooming or up sampling (default)\n",
        "- cv2.INTER_CUBIC - Better\n",
        "- cv2.INTER_LANCZOS4 - Best"
      ],
      "metadata": {
        "id": "9RhZQtDYuQL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Types of re-scaling Methods in OpenCV**\n",
        "\n",
        "- **INTER_NEAREST** – a nearest-neighbor interpolation\n",
        "- **INTER_LINEAR** – a bilinear interpolation (used by default)\n",
        "- **INTER_AREA** – resampling using pixel area relation. It may be a preferred method for image decimation, as it gives moire’-free results. But when the image is zoomed, it is similar to theINTER_NEAREST method.\n",
        "- **INTER_CUBIC** – a bicubic interpolation over 4×4 pixel neighborhood\n",
        "- **INTER_LANCZOS4** – a Lanczos interpolation over 8×8 pixel neighborhood\n",
        "\n",
        "See more on their performance - https://chadrick-kwag.net/cv2-resize-interpolation-methods/"
      ],
      "metadata": {
        "id": "xkIJ2tr6uXIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load our input image\n",
        "image = cv2.imread('./sahil/sahil_1.jpg')\n",
        "imshow(\"Scaling - Linear Interpolation\", image)\n",
        "\n",
        "# If no interpolation is specified cv.INTER_LINEAR is used as default\n",
        "# Let's make our image 3/4 of it's original size\n",
        "image_scaled = cv2.resize(image, None, fx=0.75, fy=0.75)\n",
        "imshow(\"0.75x Scaling - Linear Interpolation\", image_scaled)\n",
        "\n",
        "# Let's double the size of our image\n",
        "img_scaled2 = cv2.resize(image, None, fx=2, fy=2, interpolation = cv2.INTER_CUBIC)\n",
        "imshow(\"2x Scaling - Inter Cubic\", img_scaled2)\n",
        "\n",
        "# Let's double the size of our image using inter_nearest interpolation\n",
        "img_scaled3 = cv2.resize(image, None, fx=2, fy=2, interpolation = cv2.INTER_NEAREST)\n",
        "imshow(\"2x Scaling - Inter Nearest\", img_scaled3)\n",
        "\n",
        "# Let's skew the re-sizing by setting exact dimensions\n",
        "img_scaled4 = cv2.resize(image, (900, 400), interpolation = cv2.INTER_AREA)\n",
        "imshow(\"Scaling - Inter Area\", img_scaled4)"
      ],
      "metadata": {
        "id": "3Z68DW-PtNqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Image Pyraminds**"
      ],
      "metadata": {
        "id": "XX5MTZcBvDOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('./sahil/sahil_1.jpg')\n",
        "\n",
        "smaller = cv2.pyrDown(image)\n",
        "larger = cv2.pyrUp(smaller)\n",
        "\n",
        "imshow(\"Original\", image)\n",
        "imshow('Smaller', smaller)\n",
        "imshow('Larger', larger)\n",
        "\n",
        "even_smaller = cv2.pyrDown(smaller)\n",
        "imshow('Even Smaller', even_smaller)"
      ],
      "metadata": {
        "id": "Fu47eMm3u04r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cropping**"
      ],
      "metadata": {
        "id": "5Vo9eLFXvVL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('./sahil/sahil_1.jpg')\n",
        "\n",
        "# Get our image dimensions\n",
        "height, width = image.shape[:2]\n",
        "\n",
        "# Let's get the starting pixel coordiantes (top  left of cropping rectangle)\n",
        "# using 0.25 to get the x,y position that is 1/4 down from the top left (0,0)\n",
        "start_row, start_col = int(height * .25), int(width * .25)\n",
        "\n",
        "# Let's get the ending pixel coordinates (bottom right)\n",
        "end_row, end_col = int(height * .75), int(width * .75)\n",
        "\n",
        "# Simply use indexing to crop out the rectangle we desire\n",
        "cropped = image[start_row:end_row , start_col:end_col]\n",
        "\n",
        "imshow(\"Original Image\", image)\n",
        "\n",
        "# The cv2.rectangle function draws a rectangle over our image (in-place operation)\n",
        "copy = image.copy()\n",
        "cv2.rectangle(copy, (start_col,start_row), (end_col,end_row), (0,255,255), 10)\n",
        "\n",
        "imshow(\"Area we are cropping\", copy)\n",
        "\n",
        "imshow(\"Cropped Image\", cropped) "
      ],
      "metadata": {
        "id": "Jz3fXNLUvH0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convolutions, Blurring and Sharpening Images**\n",
        "\n",
        "1. Convolution Operations\n",
        "2. Blurring\n",
        "3. Denoising\n",
        "4. Sharpening"
      ],
      "metadata": {
        "id": "asQoC6KzvrBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Blurring using Convolutions**"
      ],
      "metadata": {
        "id": "EGKN7ovuvxOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "image = cv2.imread('./sahil/sahil_3.jpg')\n",
        "imshow('Original Image', image)\n",
        "\n",
        "# Creating our 3 x 3 kernel\n",
        "kernel_3x3 = np.ones((3, 3), np.float32) / 9\n",
        "\n",
        "# We use the cv2.fitler2D to conovlve the kernal with an image \n",
        "blurred = cv2.filter2D(image, -1, kernel_3x3)\n",
        "imshow('3x3 Kernel Blurring', blurred)\n",
        "\n",
        "# Creating our 7 x 7 kernel\n",
        "kernel_7x7 = np.ones((7, 7), np.float32) / 49\n",
        "\n",
        "blurred2 = cv2.filter2D(image, -1, kernel_7x7)\n",
        "imshow('7x7 Kernel Blurring', blurred2)"
      ],
      "metadata": {
        "id": "Gx7p3yZ6vZ69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Other commonly used blurring methods in OpenCV**\n",
        "- Regular Blurring\n",
        "- Gaussian Blurring\n",
        "- Median Blurring"
      ],
      "metadata": {
        "id": "_zokAOp7wpJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('./sahil/sahil_3.jpg')\n",
        "\n",
        "# Averaging done by convolving the image with a normalized box filter. \n",
        "# This takes the pixels under the box and replaces the central element\n",
        "# Box size needs to odd and positive \n",
        "blur = cv2.blur(image, (5,5))\n",
        "imshow('Averaging', blur)\n",
        "\n",
        "# Instead of box filter, gaussian kernel\n",
        "Gaussian = cv2.GaussianBlur(image, (5,5), 0)\n",
        "imshow('Gaussian Blurring', Gaussian)\n",
        "\n",
        "# Takes median of all the pixels under kernel area and central \n",
        "# element is replaced with this median value\n",
        "median = cv2.medianBlur(image, 5)\n",
        "imshow('Median Blurring', median)"
      ],
      "metadata": {
        "id": "5ZM7IUVtv68V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bilateral Filter**\n",
        "#### ```dst = cv.bilateralFilter(src, d, sigmaColor, sigmaSpace[, dst[, borderType]])```\n",
        "- **src**\tSource 8-bit or floating-point, 1-channel or 3-channel image.\n",
        "- **dst**\tDestination image of the same size and type as src .\n",
        "- **d**\tDiameter of each pixel neighborhood that is used during filtering. If it is non-positive, it is computed from sigmaSpace.\n",
        "- **sigmaColor**\tFilter sigma in the color space. A larger value of the parameter means that farther colors within the pixel neighborhood (see sigmaSpace) will be mixed together, resulting in larger areas of semi-equal color.\n",
        "- **sigmaSpace**\tFilter sigma in the coordinate space. A larger value of the parameter means that farther pixels will influence each other as long as their colors are close enough (see sigmaColor ). When d>0, it specifies the neighborhood size regardless of sigmaSpace. Otherwise, d is proportional to sigmaSpace.\n",
        "- **borderType**\tborder mode used to extrapolate pixels outside of the image"
      ],
      "metadata": {
        "id": "fCdWqLmqxv9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bilateral is very effective in noise removal while keeping edges sharp\n",
        "bilateral = cv2.bilateralFilter(image, 9, 75, 75)\n",
        "imshow('Bilateral Blurring', bilateral)"
      ],
      "metadata": {
        "id": "A8mBj4xwwvta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Image De-noising - Non-Local Means Denoising**\n",
        "\n",
        "**There are 4 variations of Non-Local Means Denoising:**\n",
        "\n",
        "- cv2.fastNlMeansDenoising() - works with a single grayscale images\n",
        "- cv2.fastNlMeansDenoisingColored() - works with a color image.\n",
        "- cv2.fastNlMeansDenoisingMulti() - works with image sequence captured in short period of time (grayscale images)\n",
        "- cv2.fastNlMeansDenoisingColoredMulti() - same as above, but for color images.\n",
        "\n",
        "```fastNlMeansDenoisingColored(InputArray src, OutputArray dst, float h=3, float hColor=3, int templateWindowSize=7, int searchWindowSize=21 )¶```\n",
        "\n",
        "#### Parameters for fastNlMeansDenoisingColored:\t\n",
        "\n",
        "- **src** – Input 8-bit 3-channel image.\n",
        "- **dst** – Output image with the same size and type as src .\n",
        "templateWindowSize – Size in pixels of the template patch that is used to compute weights. Should be odd. Recommended value 7 pixels\n",
        "- **searchWindowSize** – Size in pixels of the window that is used to compute weighted average for given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater denoising time. Recommended value 21 pixels\n",
        "- **h** – Parameter regulating filter strength for luminance component. Bigger h value perfectly removes noise but also removes image details, smaller h value preserves details but also preserves some noise\n",
        "- **hColor** – The same as h but for color components. For most images value equals 10 will be enought to remove colored noise and do not distort colors"
      ],
      "metadata": {
        "id": "Auay7dB7x4af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('./sahil/sahil_3.jpg')\n",
        "imshow('Original', image)\n",
        "\n",
        "dst = cv2.fastNlMeansDenoisingColored(image, None, 6, 6, 7, 21)\n",
        "imshow('fastNlMeansDenoisingColored', dst)"
      ],
      "metadata": {
        "id": "6Mj2Sc2Zx0fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sharpening Images**"
      ],
      "metadata": {
        "id": "xZFlDtzxyMGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading our image\n",
        "image = cv2.imread('./sahil/sahil_3.jpg')\n",
        "imshow('Original', image)\n",
        "\n",
        "# Create our shapening kernel, remember it must sum to one \n",
        "kernel_sharpening = np.array([[-1,-1,-1], \n",
        "                              [-1, 9,-1],\n",
        "                              [-1,-1,-1]])\n",
        "\n",
        "# applying the sharpening kernel to the image\n",
        "sharpened = cv2.filter2D(image, -1, kernel_sharpening)\n",
        "imshow('Sharpened Image', sharpened)"
      ],
      "metadata": {
        "id": "XgLVnRdcyB73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###End of Code"
      ],
      "metadata": {
        "id": "tB3y1OXtyg_g"
      }
    }
  ]
}