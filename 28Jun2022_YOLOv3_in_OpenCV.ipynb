{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "28Jun2022 YOLOv3 in OpenCV.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYHgcnDbnxLrtctr2KmdBk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahil301290/TSS2022-ThaparSummerSchool/blob/main/28Jun2022_YOLOv3_in_OpenCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YOLOv3 in using cv2.dnn.readNetFrom()**\n",
        "\n",
        "####**Load a pre-trained YOLOV3 Model and use OpenCV to run inferences over a few images**"
      ],
      "metadata": {
        "id": "3w94TWqHGs4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. Importing libraries and dataset**\n",
        "\n",
        "###**1.1 Importing libraries**"
      ],
      "metadata": {
        "id": "tpr2hx12G8cx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TC9k6R8Gltb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from matplotlib import pyplot as plt "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.2 Define our imshow function**"
      ],
      "metadata": {
        "id": "IRdiQc4BHF7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(title = \"Image\", image = None, size = 10):\n",
        "    w, h = image.shape[0], image.shape[1]\n",
        "    aspect_ratio = w/h\n",
        "    plt.figure(figsize=(size * aspect_ratio,size))\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GPzeHa6FHDrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.3 Importing dataset**"
      ],
      "metadata": {
        "id": "UXNrKlufHdAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://moderncomputervision.s3.eu-west-2.amazonaws.com/YOLO.zip\n",
        "!unzip -qq YOLO.zip"
      ],
      "metadata": {
        "id": "GTIaes0ZHUHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. YOLO Object Detection**\n",
        "\n",
        "![](https://opencv-tutorial.readthedocs.io/en/latest/_images/yolo1_net.png)\n",
        "\n",
        "**Steps Invovled**\n",
        "\n",
        "1. Use Pretrained YOLOV3 weights (237MB)- https://pjreddie.com/media/files/yolov3.weights\n",
        "2. Create our blob object which is our loaded model\n",
        "3. Set the backend that runs the model"
      ],
      "metadata": {
        "id": "epV-qMdQHsCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2.1 Exploring Object Names/Labels, YOLO weights and Layers**"
      ],
      "metadata": {
        "id": "tvPbPaaZKS7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the COCO class labels our YOLO model was trained on\n",
        "labelsPath = \"YOLO/yolo/coco.names\"\n",
        "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
        " \n",
        "# We now need to initialize a list of colors to represent each possible class label\n",
        "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")\n",
        "\n",
        "print(\"Loading YOLO weights...\") \n",
        "\n",
        "weights_path = \"YOLO/yolo/yolov3.weights\" \n",
        "cfg_path = \"YOLO/yolo/yolov3.cfg\"\n",
        "\n",
        "# Create our blob object\n",
        "net = cv2.dnn.readNetFromDarknet(cfg_path, weights_path)\n",
        "\n",
        "# Set our backend\n",
        "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
        "# net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
        "\n",
        "print(\"Our YOLO Layers\")\n",
        "ln = net.getLayerNames()\n",
        "\n",
        "# There are 254 Layers\n",
        "print(len(ln), ln)"
      ],
      "metadata": {
        "id": "6V60VE9iHih_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input to the network is a called blob object. \n",
        "\n",
        "The function ```cv.dnn.blobFromImage(img, scale, size, mean)``` transforms the image into a blob:\n",
        "\n",
        "```blob = cv.dnn.blobFromImage(img, 1/255.0, (416, 416), swapRB=True, crop=False)```\n",
        "\n",
        "**It has the following parameters:**\n",
        "\n",
        "1. the image to transform\n",
        "2. the scale factor (1/255 to scale the pixel values to [0..1])\n",
        "3. the size, here a 416x416 square image\n",
        "4. the mean value (default=0)\n",
        "5. the option swapBR=True (since OpenCV uses BGR)\n",
        "\n",
        "**Note** A blob is a 4D numpy array object (images, channels, width, height). The image below shows the red channel of the blob. You notice the brightness of the red jacket in the background."
      ],
      "metadata": {
        "id": "2n1MSt6lIErv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2.2 Detecting objects**"
      ],
      "metadata": {
        "id": "rOQBHP1JKRhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting Detections...\")\n",
        "# Get images located in ./images folder    \n",
        "mypath = \"YOLO/images/\"\n",
        "file_names = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "\n",
        "# Loop through images run them through our classifer\n",
        "for file in file_names:\n",
        "    # load our input image and grab its spatial dimensions\n",
        "    image = cv2.imread(mypath+file)\n",
        "    (H, W) = image.shape[:2]\n",
        " \n",
        "    # we want only the *output* layer names that we need from YOLO\n",
        "    ln = net.getLayerNames()\n",
        "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "    # Now we contruct our blob from our input images\n",
        "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
        "    # We set our input to our image blob\n",
        "    net.setInput(blob)\n",
        "    # Then we run a forward pass through the network\n",
        "    layerOutputs = net.forward(ln)\n",
        "\n",
        "    # we initialize our lists for our detected bounding boxes, confidences, and classes\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    IDs = []\n",
        "\n",
        "    # Loop over each of the layer outputs\n",
        "    for output in layerOutputs:\n",
        "\n",
        "        # Loop over each detection\n",
        "        for detection in output:\n",
        "            # Obtain class ID and probality of detection\n",
        "            scores = detection[5:]\n",
        "            classID = np.argmax(scores)\n",
        "            confidence = scores[classID]\n",
        "\n",
        "            # We keep only the most probably predictions\n",
        "            if confidence > 0.75:\n",
        "                # We scale the bounding box coordinates relative to the image\n",
        "                # Note: YOLO actually returns the center (x, y) of the bounding\n",
        "                # box followed by the width and height of the box\n",
        "                box = detection[0:4] * np.array([W, H, W, H])\n",
        "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "\n",
        "                # Get the the top and left corner of the bounding box\n",
        "                # Remember we alredy have the width and height\n",
        "                x = int(centerX - (width / 2))\n",
        "                y = int(centerY - (height / 2))\n",
        "\n",
        "                # Append our list of bounding box coordinates, confidences and class IDs\n",
        "                boxes.append([x, y, int(width), int(height)])\n",
        "                confidences.append(float(confidence))\n",
        "                IDs.append(classID)\n",
        "\n",
        "    # Now we apply non-maxima suppression to reduce overlapping bounding boxes\n",
        "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.3)\n",
        "\n",
        "    # We proceed once a detection has been found\n",
        "    if len(idxs) > 0:\n",
        "        # iterate over the indexes we are keeping\n",
        "        for i in idxs.flatten():\n",
        "            # Get the bounding box coordinates\n",
        "            (x, y) = (boxes[i][0], boxes[i][1])\n",
        "            (w, h) = (boxes[i][2], boxes[i][3])\n",
        "\n",
        "            # Draw our bounding boxes and put our class label on the image\n",
        "            color = [int(c) for c in COLORS[IDs[i]]]\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 3)\n",
        "            text = \"{}: {:.4f}\".format(LABELS[IDs[i]], confidences[i])\n",
        "            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    # show the output image\n",
        "    imshow(\"YOLO Detections\", image, size = 12)"
      ],
      "metadata": {
        "id": "mbOwOT0tH9UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NOTE:** **How to Perform non maximum suppression given boxes and corresponding scores.**\n",
        "\n",
        "```indices\t=\tcv.dnn.NMSBoxes(\tbboxes, scores, score_threshold, nms_threshold[, eta[, top_k]]```\n",
        "\n",
        "\n",
        "\n",
        "**Parameters**\n",
        "- bboxes\ta set of bounding boxes to apply NMS.\n",
        "- scores\ta set of corresponding confidences.\n",
        "- score_threshold\ta threshold used to filter boxes by score.\n",
        "- nms_threshold\ta threshold used in non maximum suppression.\n",
        "indices\tthe kept indices of bboxes after NMS.\n",
        "- eta\ta coefficient in adaptive threshold formula: nms_thresholdi+1=eta⋅nms_thresholdi.\n",
        "- top_k\tif >0, keep at most top_k picked indices.\n"
      ],
      "metadata": {
        "id": "jFgdQXm1IR5_"
      }
    }
  ]
}